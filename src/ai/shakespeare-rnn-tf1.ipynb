{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是学习tensorflow官网资料：https://tensorflow.google.cn/tutorials/sequences/text_generation 的笔记，通过RNN喂入莎士比亚的戏剧文本，尝试让电脑自己写出莎士比亚风格的文章。运行这个简单的例子需要强大的GPU，在我的笔记本上（MX 150只有2G显存）无法运行，如果只使用CPU需要较长的时间，需要有心理准备。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启用eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 1.x默认没有启用eager execution，因此需要明确执行`enable_eager_execution()`打开这个开关。只有1.11以上版本才支持eager execution。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载和观察数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只要使用`tf.keras`中的方法下载的数据，默认都存放到了\\$HOME/.keras/datasets目录下。下面是我的.keras/datasets目录的内容：\n",
    "```shell\n",
    "~/.keras/datasets$ ls\n",
    "auto-mpg.data            cifar-10-batches-py.tar.gz  iris_test.csv\n",
    "cifar-100-python         fashion-mnist               iris_training.csv\n",
    "cifar-100-python.tar.gz  imdb.npz                    mnist.npz\n",
    "cifar-10-batches-py      imdb_word_index.json        shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/subaochen/.keras/datasets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里不使用`tf.data.Dataset.TextlineDataset`？也许是因为需要进一步对文本进行分拆处理的缘故？\n",
    "\n",
    "也没有使用`pandas`提供的方法？\n",
    "\n",
    "有机会尝试使用`Dataset`或`pandas`改写这个部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 1000 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量化\n",
    "文本向量化才能喂入RNN学习，需要三个步骤：\n",
    "1. 构造文本字典vocab\n",
    "1. 建立字典索引char2idx，将字典的每一个字符映射为数字\n",
    "1. 使用char2idx将文本数字化（向量化）\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> 使用tf.data.Dataset.map方法可以更方便的处理文本向量化？不过就无法观察向量化文本的过程了。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text)) # sorted保证了集合的顺序\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# vocab是有序集合，转化为数组后其下标自然就是序号，但是不如char2idx结构直观\n",
    "# 如果模仿char2idx也很简单：idx2char = {i:u for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种方式观察一下向量化后的文本。这里没有使用matplotlib，没有太大意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n"
     ]
    }
   ],
   "source": [
    "# 取出char2idx前20个元素的奇怪写法。zip方法返回成对的元组，range(20)提供了序号。\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练数据（样本数据）\n",
    "把数据喂给RNN之前，需要构造/划分好训练数据和验证数据。在这里，无需验证和测试数据，因此只需要划分好训练数据即可。下面的代码中，每次喂给RNN的训练数据是seq_length个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101,) <dtype: 'int64'>\n",
      "WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(101,)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "(101,)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "(101,)\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "(101,)\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "(101,)\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "# 每次喂入RNN的字符数。注意和后面的BATCH_SIZE的区别以及匹配\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "\n",
    "# Create training examples / targets\n",
    "# drop_remainder=True：丢弃最后一个长度不足的文本块\n",
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
    "print(chunks.output_shapes,chunks.output_types)\n",
    "\n",
    "# repl函数的意义相当于Java的toString方法\n",
    "# 注意，这里的item已经是tensor了，通过numpy()方法转化为numpy矩阵（向量）\n",
    "# numpy数组（List）的强大之处：允许接受一个list作为索引参数，因此idx2char[item.numpy()]即为根据item\n",
    "# 的数字为索引获得字符构造出一个字符串\n",
    "for item in chunks.take(5):\n",
    "  print(item.shape)\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建输入文本和目标文本\n",
    "输入文本即参数，目标文本相当于“标签”，预测文本将和目标文本比较以计算误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # 不包括-1即最后一个字符，总共100个字符。这就是为什么chunk的长度是101的原因\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = chunks.map(split_input_target)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练之前，先简单模拟一下预测First这个单词的过程：比如第一步（step 0），获得输入是19（F），预测值应该是47（i），以此类推。当然，这不是RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批次重新构造训练数据\n",
    "chunks已经是通过批次（batch）获取的字符串了，这里进一步缩小批次的范围不知何意？为什么不一次batch到位？\n",
    "\n",
    "到目前为止，使用了如下的变量来表示文本的不同形态：\n",
    "* text: 原始的文本\n",
    "* text_as_int：向量化（数字化）的字符串\n",
    "* chunks：按照seq_length+1切分的Dataset\n",
    "* dataset：划分为input_text和target_text的Dataset，此时的dataset其实比chunks大了一倍\n",
    "\n",
    "？seq_length和BATCH_SIZE的关系是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "# 由于seq_length = 100，这里的32会造成每个输入丢失4-5个字符（5%的输入丢失）。\n",
    "BATCH_SIZE = 32\n",
    "# steps_per_epoch = len(text)//seq_length//BATCH_SIZE\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# shuffle会造成字符顺序的混乱，在这里是否有道理？\n",
    "# 这里的dataset并没有改变数据量的大小，只是按照BATCH_SIZE进行重新划分\n",
    "# 也就是说，input_text和target_text都按照BATCH_SIZE重新划分，每个批次取出一个input_text\n",
    "# 和target_text进行训练（BATCH_SIZE大小）\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型\n",
    "\n",
    "模型分为三层：\n",
    "1. 嵌入层（layers.Embedding)。关于嵌入的概念可参考：https://tensorflow.google.cn/guide/embedding 。简单的说，嵌入层的作用是将输入(本例是输入字符的索引)映射为一个高维度向量（dense vector），其好处是可以借助于向量的方法，比如欧氏距离或者角度来度量两个向量的相似性。对于文本而言，就是两个词的相似度。\n",
    "2. GRU层（Gated Recurrent Unit）\n",
    "3. 全链接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置模型参数，实例化模型\n",
    "为了能够在笔记本电脑上运行，特意调小了embedding_dim和rnn_units两个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "#embedding_dim = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "#rnn_units = 1024\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先测试一下模型\n",
    "两个问题：\n",
    "* model(input_example_batch)这种调用方式是什么意思？\n",
    "* model的返回值如何确定？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (32, None, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 65)            16705     \n",
      "=================================================================\n",
      "Total params: 428,097\n",
      "Trainable params: 428,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 38, 55, 13, 61, 33, 37, 56,  6, 34,  3, 19, 10,  6, 49,  9,  9,\n",
       "       46, 40, 12,  5,  1,  0, 61,  7, 26,  1, 11, 41, 62,  9, 29, 47, 37,\n",
       "        3, 56, 33, 52,  1, 41, 57, 63, 18, 26, 13, 46, 34, 22, 23, 39,  1,\n",
       "       61,  6, 57, 32, 45,  8,  9, 13,  9, 51, 20, 19, 20, 57, 45, 15, 23,\n",
       "       63,  0, 62, 28, 30, 63, 55,  2, 13, 42, 48,  5, 38, 16, 36,  3,  1,\n",
       "       17, 13, 58, 51, 12, 14, 45, 21, 57,  8, 44,  3, 57, 63, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"ay:\\nYour hand, my Perdita: so turtles pair,\\nThat never mean to part.\\n\\nPERDITA:\\nI'll swear for 'em.\\n\\n\"\n",
      "\n",
      "Next Char Predictions: \n",
      " \"VZqAwUYr,V$G:,k33hb?' \\nw-N ;cx3QiY$rUn csyFNAhVJKa w,sTg.3A3mHGHsgCKy\\nxPRyq!Adj'ZDX$ EAtm?BgIs.f$syz\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.174056\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恢复checkpoint\n",
    "如何检测checkoutpoint是否存在？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if ckpt != None:\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
    "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "  model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "346/348 [============================>.] - ETA: 0s - loss: 2.3941WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "348/348 [==============================] - 18s 52ms/step - loss: 2.3917\n",
      "Epoch 2/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.8437\n",
      "Epoch 3/300\n",
      "348/348 [==============================] - 15s 42ms/step - loss: 1.6631\n",
      "Epoch 4/300\n",
      "348/348 [==============================] - 13s 39ms/step - loss: 1.5713\n",
      "Epoch 5/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.5140\n",
      "Epoch 6/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.4741\n",
      "Epoch 7/300\n",
      "348/348 [==============================] - 14s 39ms/step - loss: 1.4469\n",
      "Epoch 8/300\n",
      "348/348 [==============================] - 15s 42ms/step - loss: 1.4238\n",
      "Epoch 9/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.4074\n",
      "Epoch 10/300\n",
      "348/348 [==============================] - 14s 39ms/step - loss: 1.3941\n",
      "Epoch 11/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.3804\n",
      "Epoch 12/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3707\n",
      "Epoch 13/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3615\n",
      "Epoch 14/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3525\n",
      "Epoch 15/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3449\n",
      "Epoch 16/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3393\n",
      "Epoch 17/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3317\n",
      "Epoch 18/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3271\n",
      "Epoch 19/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3220\n",
      "Epoch 20/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3167\n",
      "Epoch 21/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3102\n",
      "Epoch 22/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.3076\n",
      "Epoch 23/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.3020\n",
      "Epoch 24/300\n",
      "348/348 [==============================] - 13s 39ms/step - loss: 1.2975\n",
      "Epoch 25/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2957\n",
      "Epoch 26/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2911\n",
      "Epoch 27/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2881\n",
      "Epoch 28/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2849\n",
      "Epoch 29/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2802\n",
      "Epoch 30/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2783\n",
      "Epoch 31/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2762\n",
      "Epoch 32/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2718\n",
      "Epoch 33/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2693\n",
      "Epoch 34/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2683\n",
      "Epoch 35/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2639\n",
      "Epoch 36/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2627\n",
      "Epoch 37/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2599\n",
      "Epoch 38/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2582\n",
      "Epoch 39/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2534\n",
      "Epoch 40/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2547\n",
      "Epoch 41/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2522\n",
      "Epoch 42/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2511\n",
      "Epoch 43/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2470\n",
      "Epoch 44/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2460\n",
      "Epoch 45/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2443\n",
      "Epoch 46/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2445\n",
      "Epoch 47/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2399\n",
      "Epoch 48/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2396\n",
      "Epoch 49/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2379\n",
      "Epoch 50/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2366\n",
      "Epoch 51/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2366\n",
      "Epoch 52/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2334\n",
      "Epoch 53/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2339\n",
      "Epoch 54/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2293\n",
      "Epoch 55/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2311\n",
      "Epoch 56/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2294\n",
      "Epoch 57/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2287\n",
      "Epoch 58/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2277\n",
      "Epoch 59/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2263\n",
      "Epoch 60/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2245\n",
      "Epoch 61/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2237\n",
      "Epoch 62/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2245\n",
      "Epoch 63/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2237\n",
      "Epoch 64/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2223\n",
      "Epoch 65/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2214\n",
      "Epoch 66/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2193\n",
      "Epoch 67/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2184\n",
      "Epoch 68/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2189\n",
      "Epoch 69/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2163\n",
      "Epoch 70/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2174\n",
      "Epoch 71/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2172\n",
      "Epoch 72/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2164\n",
      "Epoch 73/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2151\n",
      "Epoch 74/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2158\n",
      "Epoch 75/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2101\n",
      "Epoch 76/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2148\n",
      "Epoch 77/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2135\n",
      "Epoch 78/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2110\n",
      "Epoch 79/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2134\n",
      "Epoch 80/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2112\n",
      "Epoch 81/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2112\n",
      "Epoch 82/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2094\n",
      "Epoch 83/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2090\n",
      "Epoch 84/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2094\n",
      "Epoch 85/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2088\n",
      "Epoch 86/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2091\n",
      "Epoch 87/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2074\n",
      "Epoch 88/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2082\n",
      "Epoch 89/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2075\n",
      "Epoch 90/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2070\n",
      "Epoch 91/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2082\n",
      "Epoch 92/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2062\n",
      "Epoch 93/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2054\n",
      "Epoch 94/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2048\n",
      "Epoch 95/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2063\n",
      "Epoch 96/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2049\n",
      "Epoch 97/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2059\n",
      "Epoch 98/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2041\n",
      "Epoch 99/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2026\n",
      "Epoch 100/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2054\n",
      "Epoch 101/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2027\n",
      "Epoch 102/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2030\n",
      "Epoch 103/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2031\n",
      "Epoch 104/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2027\n",
      "Epoch 105/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.2004\n",
      "Epoch 106/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2017\n",
      "Epoch 107/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2022\n",
      "Epoch 108/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2014\n",
      "Epoch 109/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2019\n",
      "Epoch 110/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2008\n",
      "Epoch 111/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2013\n",
      "Epoch 112/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2012\n",
      "Epoch 113/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2003\n",
      "Epoch 114/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2006\n",
      "Epoch 115/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.2000\n",
      "Epoch 116/300\n",
      "348/348 [==============================] - 18s 51ms/step - loss: 1.1993\n",
      "Epoch 117/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1995\n",
      "Epoch 118/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.2007\n",
      "Epoch 119/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1992\n",
      "Epoch 120/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1997\n",
      "Epoch 121/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1972\n",
      "Epoch 122/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1996\n",
      "Epoch 123/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1975\n",
      "Epoch 124/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1986\n",
      "Epoch 125/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1976\n",
      "Epoch 126/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1964\n",
      "Epoch 127/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1974\n",
      "Epoch 128/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1987\n",
      "Epoch 129/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1965\n",
      "Epoch 130/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1958\n",
      "Epoch 131/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1984\n",
      "Epoch 132/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1969\n",
      "Epoch 133/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1967\n",
      "Epoch 134/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1976\n",
      "Epoch 135/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1958\n",
      "Epoch 136/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1953\n",
      "Epoch 137/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1970\n",
      "Epoch 138/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1962\n",
      "Epoch 139/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1968\n",
      "Epoch 140/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1953\n",
      "Epoch 141/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1960\n",
      "Epoch 142/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1959\n",
      "Epoch 143/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1947\n",
      "Epoch 144/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1952\n",
      "Epoch 145/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1954\n",
      "Epoch 146/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1947\n",
      "Epoch 147/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1948\n",
      "Epoch 148/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1929\n",
      "Epoch 149/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1948\n",
      "Epoch 150/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1945\n",
      "Epoch 151/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1949\n",
      "Epoch 152/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1952\n",
      "Epoch 153/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1938\n",
      "Epoch 154/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1927\n",
      "Epoch 155/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1934\n",
      "Epoch 156/300\n",
      "348/348 [==============================] - 13s 39ms/step - loss: 1.1929\n",
      "Epoch 157/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1966\n",
      "Epoch 158/300\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.1927\n",
      "Epoch 159/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1930\n",
      "Epoch 160/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1947\n",
      "Epoch 161/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1929\n",
      "Epoch 162/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1951\n",
      "Epoch 163/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1911\n",
      "Epoch 164/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1930\n",
      "Epoch 165/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1945\n",
      "Epoch 166/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1921\n",
      "Epoch 167/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1923\n",
      "Epoch 168/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1930\n",
      "Epoch 169/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1926\n",
      "Epoch 170/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1928\n",
      "Epoch 171/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1933\n",
      "Epoch 172/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1925\n",
      "Epoch 173/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1915\n",
      "Epoch 174/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1928\n",
      "Epoch 175/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1912\n",
      "Epoch 176/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1927\n",
      "Epoch 177/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1921\n",
      "Epoch 178/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1915\n",
      "Epoch 179/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1904\n",
      "Epoch 180/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1942\n",
      "Epoch 181/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1926\n",
      "Epoch 182/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1915\n",
      "Epoch 183/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1907\n",
      "Epoch 184/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1930\n",
      "Epoch 185/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1893\n",
      "Epoch 186/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1896\n",
      "Epoch 187/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1951\n",
      "Epoch 188/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1903\n",
      "Epoch 189/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1898\n",
      "Epoch 190/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1908\n",
      "Epoch 191/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1922\n",
      "Epoch 192/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1909\n",
      "Epoch 193/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1885\n",
      "Epoch 194/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1915\n",
      "Epoch 195/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1914\n",
      "Epoch 196/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1896\n",
      "Epoch 197/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1911\n",
      "Epoch 198/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1905\n",
      "Epoch 199/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1903\n",
      "Epoch 200/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1892\n",
      "Epoch 201/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1912\n",
      "Epoch 202/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1902\n",
      "Epoch 203/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1913\n",
      "Epoch 204/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1881\n",
      "Epoch 205/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1910\n",
      "Epoch 206/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1885\n",
      "Epoch 207/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1897\n",
      "Epoch 208/300\n",
      "348/348 [==============================] - 13s 36ms/step - loss: 1.1904\n",
      "Epoch 209/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1869\n",
      "Epoch 210/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1896\n",
      "Epoch 211/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1890\n",
      "Epoch 212/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1896\n",
      "Epoch 213/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1901\n",
      "Epoch 214/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1910\n",
      "Epoch 215/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1866\n",
      "Epoch 216/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1878\n",
      "Epoch 217/300\n",
      "348/348 [==============================] - ETA: 0s - loss: 1.189 - 13s 37ms/step - loss: 1.1898\n",
      "Epoch 218/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1893\n",
      "Epoch 219/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1872\n",
      "Epoch 220/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1885\n",
      "Epoch 221/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1876\n",
      "Epoch 222/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1884\n",
      "Epoch 223/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1880\n",
      "Epoch 224/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1886\n",
      "Epoch 225/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1883\n",
      "Epoch 226/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1886\n",
      "Epoch 227/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1880\n",
      "Epoch 228/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1868\n",
      "Epoch 229/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1883\n",
      "Epoch 230/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1878\n",
      "Epoch 231/300\n",
      "348/348 [==============================] - 17s 50ms/step - loss: 1.1886\n",
      "Epoch 232/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1887\n",
      "Epoch 233/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1885\n",
      "Epoch 234/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1871\n",
      "Epoch 235/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1884\n",
      "Epoch 236/300\n",
      "348/348 [==============================] - 12s 35ms/step - loss: 1.1876\n",
      "Epoch 237/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1876\n",
      "Epoch 238/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1876\n",
      "Epoch 239/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1874\n",
      "Epoch 240/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1867\n",
      "Epoch 241/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1879\n",
      "Epoch 242/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1888\n",
      "Epoch 243/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1867\n",
      "Epoch 244/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1876\n",
      "Epoch 245/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1868\n",
      "Epoch 246/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1880\n",
      "Epoch 247/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1857\n",
      "Epoch 248/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1893\n",
      "Epoch 249/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1871\n",
      "Epoch 250/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1869\n",
      "Epoch 251/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1849\n",
      "Epoch 252/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1875\n",
      "Epoch 253/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1858\n",
      "Epoch 254/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1867\n",
      "Epoch 255/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1878\n",
      "Epoch 256/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1852\n",
      "Epoch 257/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1881\n",
      "Epoch 258/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1862\n",
      "Epoch 259/300\n",
      "348/348 [==============================] - 13s 38ms/step - loss: 1.1893\n",
      "Epoch 260/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1852\n",
      "Epoch 261/300\n",
      "348/348 [==============================] - 13s 37ms/step - loss: 1.1872\n",
      "Epoch 262/300\n",
      "203/348 [================>.............] - ETA: 3s - loss: 1.1878"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制训练图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss=history_dict['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恢复到最新的checkpoint\n",
    "\n",
    "这个步骤是不是应该放在训练之前，以便积累训练的成果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 10\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  print(input_eval)\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval.shape)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      \n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
