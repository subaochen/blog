{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "# import talos as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "os.environ['TZ'] = 'Asia/Shanghai'  # to set timezone; needed when running on cloud\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 10,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 60\n",
    "}\n",
    "\n",
    "iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "DATA_FILE=\"ge.us.txt\"\n",
    "PATH_TO_DRIVE_ML_DATA=\"./\"\n",
    "INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/inputs\"\n",
    "OUTPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/outputs/lstm_best_7-3-19_12AM/\"+iter_changes\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory recreated .//outputs/lstm_best_7-3-19_12AM/dropout_layers_0.4_0.4\n"
     ]
    }
   ],
   "source": [
    "# check if directory already exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory created\", OUTPUT_PATH)\n",
    "else:\n",
    "    os.rename(OUTPUT_PATH, OUTPUT_PATH+str(stime))\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory recreated\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ge.us.txt']\n"
     ]
    }
   ],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")\n",
    "\n",
    "\n",
    "def trim_dataset(mat,batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "#         if i < 10:\n",
    "#           print(i,\"-->\", x[i,-1,:], y[i])\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "stime = time.time()\n",
    "print(os.listdir(INPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date   Open    High    Low  Close     Volume  OpenInt\n",
      "14053  2017-11-06  20.52  20.530  20.08  20.13   60641787        0\n",
      "14054  2017-11-07  20.17  20.250  20.12  20.21   41622851        0\n",
      "14055  2017-11-08  20.21  20.320  20.07  20.12   39672190        0\n",
      "14056  2017-11-09  20.04  20.071  19.85  19.99   50831779        0\n",
      "14057  2017-11-10  19.98  20.680  19.90  20.49  100698474        0\n",
      "Date        object\n",
      "Open       float64\n",
      "High       float64\n",
      "Low        float64\n",
      "Close      float64\n",
      "Volume       int64\n",
      "OpenInt      int64\n",
      "dtype: object\n",
      "Train--Test size 11246 2812\n",
      "Deleting unused dataframes of total size(KB) 3267\n",
      "Are any NaNs present in train/test matrices? False False\n",
      "dim_0 11186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afd8427af8347f682ad514743a968b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11186), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (11186, 60, 5) (11186,)\n",
      "Batch trimmed x_t size (11180, 60, 5)\n",
      "Batch trimmed y_t size (11180,)\n"
     ]
    }
   ],
   "source": [
    "df_ge = pd.read_csv(os.path.join(INPUT_PATH, DATA_FILE), engine='python')\n",
    "#print(\"csv file shape:\",df_ge.shape)\n",
    "#print(df_ge.columns)\n",
    "print(df_ge.tail())\n",
    "tqdm_notebook.pandas('Processing...')\n",
    "# df_ge = process_dataframe(df_ge)\n",
    "print(df_ge.dtypes)\n",
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train--Test size\", len(df_train), len(df_test))\n",
    "\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "\n",
    "print(\"Deleting unused dataframes of total size(KB)\",(sys.getsizeof(df_ge)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n",
    "\n",
    "del df_ge\n",
    "del df_test\n",
    "del df_train\n",
    "del x\n",
    "\n",
    "print(\"Are any NaNs present in train/test matrices?\",np.isnan(x_train).any(), np.isnan(x_train).any())\n",
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed x_t size\",x_t.shape)\n",
    "print(\"Batch trimmed y_t size\",y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model...\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(60, dropout=0.0))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
    "    print(\"Loaded saved model...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备测试数据和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 2752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8a6693dad644d29ea1816bd0acde98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2752), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (2752, 60, 5) (2752,)\n",
      "Test size (1375, 60, 5) (1375,) (1375, 60, 5) (1375,)\n"
     ]
    }
   ],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE对执行速度影响很大，以下是一些测试结果：\n",
    "\n",
    "|BATCH_SIZE|时间（s/epoch)|\n",
    "|------|------|\n",
    "|20|140|\n",
    "|512|5|\n",
    "\n",
    "但是，过大的batch_size会影响预测结果，参见：https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_update_model = True\n",
    "if model is None or is_update_model:\n",
    "    from keras import backend as K\n",
    "    print(\"Building model...\")\n",
    "    print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "    model = create_model()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=1, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(\"lstm_model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(x_test_t, y_test_t, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3939796 ]\n",
      " [0.39360842]\n",
      " [0.39401534]\n",
      " ...\n",
      " [0.46421072]\n",
      " [0.4603142 ]\n",
      " [0.4568    ]]\n",
      "Error is 0.0017593669308608355 (1370,) (1370,)\n",
      "[0.3939796  0.39360842 0.39401534 0.3945469  0.3949426  0.39498603\n",
      " 0.3943264  0.39298123 0.39229453 0.39010546 0.3862075  0.38169855\n",
      " 0.37861767 0.3768059  0.376118  ]\n",
      "[0.33031465 0.32943225 0.33205846 0.32659593 0.32747834 0.31770881\n",
      " 0.31092267 0.31258244 0.32378063 0.32592362 0.32947426 0.32783551\n",
      " 0.33443256 0.33249967 0.3386135 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "print(y_pred)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.21126  19.193594 19.212961 19.238262 19.257097 19.259163 19.227766\n",
      " 19.16374  19.131056 19.026863 18.841331 18.626719 18.48008  18.393845\n",
      " 18.361103]\n",
      "[16.181 16.139 16.264 16.004 16.046 15.581 15.258 15.337 15.87  15.972\n",
      " 16.141 16.063 16.377 16.285 16.576]\n"
     ]
    }
   ],
   "source": [
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0ecb9b1b587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred, real):\n",
    "    \"\"\"绘制预测和实际的比较图\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(pred)\n",
    "    plt.plot(real)\n",
    "    plt.title('Prediction vs Real Stock Price')\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f92b4d62668>\n",
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c464141ccf31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrim_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0my_test_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# load the saved best model from above\n",
    "#saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) # , \"lstm_best_7-3-19_12AM\",\n",
    "saved_model = model\n",
    "print(saved_model)\n",
    "\n",
    "x_test_t = x_test_t[-1:]\n",
    "y_test_t = y_test_t[-1:]\n",
    "\n",
    "y_pred = saved_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "print(y_pred)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])\n",
    "\n",
    "# Visualize the prediction\n",
    "plot_pred(y_pred_org, y_test_t_org)\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))\n",
    "print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_90 = y_pred_org[:90]\n",
    "y_test_t_90 = y_test_t_org[:90]\n",
    "plot_pred(y_pred_90,y_test_t_90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
